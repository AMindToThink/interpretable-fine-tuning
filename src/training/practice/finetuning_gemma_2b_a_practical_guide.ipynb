{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on https://medium.com/@heyamit10/fine-tuning-gemma-2b-a-practical-guide-e4c25de43b2d "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TMPDIR\"] = \"./mytempdir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 388\n",
      "drwxrwxr-x 3 cs29824 cs29824   4096 Feb 19 22:06 .\n",
      "drwxrwxr-x 5 cs29824 cs29824   4096 Feb 19 16:40 ..\n",
      "-rw-rw-r-- 1 cs29824 cs29824   1638 Feb 19 06:05 alpaca_trainer.py\n",
      "-rw-rw-r-- 1 cs29824 cs29824   1540 Feb 19 06:37 assistant_trainer.py\n",
      "-rw-rw-r-- 1 cs29824 cs29824   1079 Feb 19 05:59 code_alpaca_trainer.py\n",
      "-rw-rw-r-- 1 cs29824 cs29824  24575 Feb 19 21:31 finetuning_gemma_2b_a_practical_guide.ipynb\n",
      "-rw-rw-r-- 1 cs29824 cs29824   1916 Feb 19 06:08 gpt_trainer.py\n",
      "drwxrwxr-x 2 cs29824 cs29824   4096 Feb 19 22:06 mytempdir\n",
      "-rw-rw-r-- 1 cs29824 cs29824    806 Feb 19 06:02 qa_trainer.py\n",
      "-rw-rw-r-- 1 cs29824 cs29824 330091 Feb 19 21:23 sft_finetuning_example.ipynb\n",
      "-rw-rw-r-- 1 cs29824 cs29824   6665 Feb 19 06:45 sft_finetuning_example.py\n"
     ]
    }
   ],
   "source": [
    "!ls . -al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")\n",
    "\n",
    "# Tokenize your dataset\n",
    "def tokenize_data(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"], truncation=True, padding=\"max_length\", max_length=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"sentence-transformers/eli5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "    output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = raw_dataset.map(\n",
    "    lambda examples: {\"text\": formatting_prompts_func(examples)[0]},\n",
    "    remove_columns=raw_dataset[\"train\"].column_names\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = raw_dataset.map(tokenize_data, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"### Question: in football whats the point of wasting the first two plays with a rush - up the middle - not regular rush plays i get those\\n ### Answer: Keep the defense honest, get a feel for the pass rush, open up the passing game. An offense that's too one dimensional will fail. And those rushes up the middle can be busted wide open sometimes for big yardage.\"}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a Pandas DataFrame\n",
    "df = tokenized_dataset[\"train\"].to_pandas()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 20:49:48.269497: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-25 20:49:48.303294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740516588.332866  664049 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740516588.340716  664049 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-25 20:49:48.368858: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bedff234ba541e2b5735e722937cf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     2,  36422, 235290, 110748, 137061, 235248, 235284, 235305,    603,\n",
      "          17305, 235341]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_name = 'google/gemma-2-2b'\n",
    "device = 'cuda:0'\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "       model_name,\n",
    "       torch_dtype=torch.float16\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Example tokenization\n",
    "text = \"Fine-tuning Gemma 2B is exciting!\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated output:\n",
      "What's the derivative of tan(x)?\n",
      "\n",
      "What\n"
     ]
    }
   ],
   "source": [
    "# Define the input string\n",
    "input_text = \"What's the derivative of tan(x)?\"\n",
    "\n",
    "# Tokenize the input text and send data to the device ('cuda:0' in our case)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate output tokens from the model\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(**inputs, max_new_tokens=2)\n",
    "\n",
    "# Decode the generated tokens back into text\n",
    "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the output string\n",
    "print(\"Generated output:\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b6641ba7e34056a3a8968906eca55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "no_quantization_pipe = pipeline(\"text-generation\", model=\"google/gemma-2-2b\", device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation is slow the first time, but then fast? As in, 2 tokens the first time takes 1 minute, but 30 tokens the third time takes .8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"What's the derivative of tan(x)?\\n\\n[Answer 1\"}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_quantization_pipe(\"What's the derivative of tan(x)?\", max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDAGraph supports dynamic shapes by recording a new graph for each distinct input size. Recording too many CUDAGraphs may lead to extra overhead. We have observed 51 distinct sizes. Please consider the following options for better performance: a) padding inputs to a few fixed number of shapes; or b) set torch._inductor.config.triton.cudagraph_skip_dynamic_graphs=True. Set torch._inductor.config.triton.cudagraph_dynamic_shape_warn_limit=None to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"How can I purchase a spear?\\n\\n[User 0001]\\n\\nI'm looking to purchase a spear. I'm not sure what to look for. I\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_quantization_pipe(\"How can I purchase a spear?\", max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"What's the derivative of tan(x)?\\n\\n[Answer 1\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"What's the derivative of tan(x)?\", max_new_tokens=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "wandb.init(project=\"gemma2b-fine-tune\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp_arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
