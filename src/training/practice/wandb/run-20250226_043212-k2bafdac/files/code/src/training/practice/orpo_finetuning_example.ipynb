{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_Yuy55M-LF-"
      },
      "source": [
        "# Preference Alignment with Odds Ratio Preference Optimization (ORPO)\n",
        "\n",
        "This notebook will guide you through the process of fine-tuning a language model using Odds Ratio Preference Optimization (ORPO). We will use the SmolLM2-135M model which has **not** been through SFT training, so it is not compatible with DPO. This means, you cannot use the model you trained in [1_instruction_tuning](../../1_instruction_tuning/notebooks/sft_finetuning_example.ipynb).\n",
        "\n",
        "<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n",
        "     <h2 style='margin: 0;color:blue'>Exercise: Aligning SmolLM2 with ORPOTrainer</h2>\n",
        "     <p>Take a dataset from the Hugging Face hub and align a model on it. </p>\n",
        "     <p><b>Difficulty Levels</b></p>\n",
        "     <p>üê¢ Use the `trl-lib/ultrafeedback_binarized` dataset</p>\n",
        "     <p>üêï Try out the `argilla/ultrafeedback-binarized-preferences` dataset</p>\n",
        "     <p>ü¶Å Try on a subset of mlabonne's `orpo-dpo-mix-40k` dataset</p>\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hkHn6rQ-LF_"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwNxh0pCQhfI",
        "outputId": "e4c288e1-2fa3-4717-d9dd-ae3ec0b91e3b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
        "\n",
        "# Authenticate to Hugging Face\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(token=input(\"Please give your huggingface key: \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logged in as: {'type': 'user', 'id': '66f9ceca46d413c380d53ff3', 'name': 'AMindToThink', 'fullname': 'Matthew Khoriaty', 'email': 'matthewkhoriaty@gmail.com', 'emailVerified': True, 'canPay': False, 'periodEnd': None, 'isPro': False, 'avatarUrl': '/avatars/44baa1c81b0e87ac5ff27c43173423d4.svg', 'orgs': [], 'auth': {'type': 'access_token', 'accessToken': {'displayName': 'zach_AMindToThink_write', 'role': 'write', 'createdAt': '2025-02-25T21:02:51.453Z'}}}\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import whoami\n",
        "try:\n",
        "    username = whoami()\n",
        "    print(f\"Logged in as: {username}\")\n",
        "except Exception as e:\n",
        "    print(\"Not logged in or error retrieving user information.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbtXToR--LGA"
      },
      "source": [
        "## Format dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AQT08S8j-LGA",
        "outputId": "23d86e61-f7ba-4d65-92be-7382f4d4d7f3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "611ac0c9ecb447188010f3eb84741218",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/643 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load dataset\n",
        "\n",
        "# TODO: ü¶Åüêï change the dataset to one of your choosing\n",
        "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5YJ0Ttm-LGB"
      },
      "outputs": [],
      "source": [
        "# TODO: üêï If your dataset is not represented as conversation lists, you can use the `process_dataset` function to convert it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKzUyZOI-LGB"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "w2kGBNStQoUd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39c16279ac0f4029836ecdf95c4f6c4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1a37ec57bee43ec921f15f8cf15bc13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b920d0be5fd4e4aa90ebc9f28cc61d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e80af702ba91412fbc8e1999f2df16eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f1771be95b440218b1b8ff6bb695ebc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86bb450601844dbdbb85a1e42698435c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7a73b94b67147f6b17d6c266157c305",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b1a0f1cf5f3443f8ec49ade4fceb0ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        ")\n",
        "assert device == 'cuda'\n",
        "# Model to fine-tune\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    torch_dtype=torch.float32,\n",
        ").to(device)\n",
        "model.config.use_cache = False\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)\n",
        "\n",
        "# Set our name for the finetune to be saved &/ uploaded to\n",
        "finetune_name = \"SmolLM2-FT-ORPO_example\"\n",
        "finetune_tags = [\"smol-course\", \"module_1\"]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jjmt0Pc1-LGB"
      },
      "source": [
        "## Train model with ORPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AWDwJe7_Qqgb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cs29824/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "orpo_args = ORPOConfig(\n",
        "    # Small learning rate to prevent catastrophic forgetting\n",
        "    learning_rate=8e-6,\n",
        "    # Linear learning rate decay over training\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    # Maximum combined length of prompt + completion\n",
        "    max_length=1024,\n",
        "    # Maximum length for input prompts\n",
        "    max_prompt_length=512,\n",
        "    # Controls weight of the odds ratio loss (Œª in paper)\n",
        "    beta=0.1,\n",
        "    # Batch size for training\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    # Helps with training stability by accumulating gradients before updating\n",
        "    gradient_accumulation_steps=4,\n",
        "    # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
        "    optim=\"paged_adamw_8bit\" if device == \"cuda\" else \"adamw_torch\",\n",
        "    # Number of training epochs\n",
        "    num_train_epochs=1,\n",
        "    # When to run evaluation\n",
        "    evaluation_strategy=\"steps\",\n",
        "    # Evaluate every 20% of training\n",
        "    eval_steps=0.2,\n",
        "    # Log metrics every step\n",
        "    logging_steps=1,\n",
        "    # Gradual learning rate warmup\n",
        "    warmup_steps=10,\n",
        "    # Disable external logging\n",
        "    report_to=\"none\",\n",
        "    # Where to save model/checkpoints\n",
        "    output_dir=\"./results/\",\n",
        "    # Enable MPS (Metal Performance Shaders) if available\n",
        "    use_mps_device=device == \"mps\",\n",
        "    hub_model_id=finetune_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JUTYdQmf-LGB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/cs29824/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:275: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a986ce7d881a4105869c7fd51c3bb631",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/62135 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25051552f18b414fb13b6e964e7d610b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/62135 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6c3c6c5941a4068821b550fbf3dda4c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/62135 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1067c994f665476d93092fdc9c01a0a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b401ff18a2e44158353b05149ce0c3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bbb6d0c78db54ac6918cdfa97c173456",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "trainer = ORPOTrainer(\n",
        "    model=model,\n",
        "    args=orpo_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    processing_class=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4mRSTqv6-LGC"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Trainer tried to instantiate bnb optimizer but `bitsandbytes` is not installed!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformers/trainer.py:1473\u001b[0m, in \u001b[0;36mTrainer.get_optimizer_cls_and_kwargs\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdamW, Lion, RMSprop\n\u001b[1;32m   1475\u001b[0m     is_paged \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinetune_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformers/trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2242\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformers/trainer.py:2321\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler \u001b[38;5;241m=\u001b[39m deepspeed_init(\u001b[38;5;28mself\u001b[39m, num_training_steps\u001b[38;5;241m=\u001b[39mmax_steps)\n\u001b[1;32m   2320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m delay_optimizer_creation:\n\u001b[0;32m-> 2321\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_optimizer_and_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_training_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m TrainerState(\n\u001b[1;32m   2324\u001b[0m     stateful_callbacks\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   2325\u001b[0m         cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cb, ExportableState)\n\u001b[1;32m   2326\u001b[0m     ]\n\u001b[1;32m   2327\u001b[0m )\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mis_hyper_param_search \u001b[38;5;241m=\u001b[39m trial \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformers/trainer.py:1177\u001b[0m, in \u001b[0;36mTrainer.create_optimizer_and_scheduler\u001b[0;34m(self, num_training_steps)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_optimizer_and_scheduler\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_training_steps: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;124;03m    Setup the optimizer and the learning rate scheduler.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m    `create_scheduler`) in a subclass.\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1177\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m IS_SAGEMAKER_MP_POST_1_10 \u001b[38;5;129;01mand\u001b[39;00m smp\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mfp16:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;66;03m# If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m         optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39moptimizer\n",
            "File \u001b[0;32m~/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformers/trainer.py:1225\u001b[0m, in \u001b[0;36mTrainer.create_optimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1223\u001b[0m     optimizer_cls, optimizer_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_cls_and_kwargs\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1225\u001b[0m     optimizer_cls, optimizer_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_optimizer_cls_and_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;66;03m# Overwrite `params` in case it's created by `get_optimizer_cls_and_kwargs`\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;66;03m# e.g. for GaLore optimizer.\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m optimizer_kwargs:\n",
            "File \u001b[0;32m~/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformers/trainer.py:1528\u001b[0m, in \u001b[0;36mTrainer.get_optimizer_cls_and_kwargs\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     optimizer_kwargs\u001b[38;5;241m.\u001b[39mupdate(bnb_kwargs)\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m-> 1528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer tried to instantiate bnb optimizer but `bitsandbytes` is not installed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bitsandbytes_available() \u001b[38;5;129;01mand\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m   1530\u001b[0m     importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1531\u001b[0m ) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.41.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1532\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1533\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using 8-bit optimizers with a version of `bitsandbytes` < 0.41.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is recommended to update your version as a major bug has been fixed in 8-bit optimizers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m     )\n",
            "\u001b[0;31mValueError\u001b[0m: Trainer tried to instantiate bnb optimizer but `bitsandbytes` is not installed!"
          ]
        }
      ],
      "source": [
        "trainer.train()  # Train the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save the model\n",
        "trainer.save_model(f\"./{finetune_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Save to the huggingface hub if login (HF_TOKEN is set)\n",
        "if os.getenv(\"HF_TOKEN\"):\n",
        "    trainer.push_to_hub(tags=finetune_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVCymxp_-LGC"
      },
      "source": [
        "## üíê You're done!\n",
        "\n",
        "This notebook provided a step-by-step guide to fine-tuning the `HuggingFaceTB/SmolLM2-135M` model using the `ORPOTrainer`. By following these steps, you can adapt the model to perform specific tasks more effectively. If you want to carry on working on this course, here are steps you could try out:\n",
        "\n",
        "- Try this notebook on a harder difficulty\n",
        "- Review a colleagues PR\n",
        "- Improve the course material via an Issue or PR."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
