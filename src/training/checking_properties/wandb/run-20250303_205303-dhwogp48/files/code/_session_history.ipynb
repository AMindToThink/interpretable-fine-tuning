{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f58093fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "# YOU HAVE TO SET CUDA_VISIBLE_DEVICES BEFORE DOING ANY IMPORTS OF cuda-related packages! https://discuss.pytorch.org/t/setting-visible-devices-with-distributed-data-parallel/93230\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true' # if the process hangs, turn this to false.\n",
    "import torch\n",
    "print(torch.cuda.device_count())  # Should print 1, but doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68273f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import libraries\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from sae_lens import HookedSAETransformer, SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7acb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import our custom ISAERFT components\n",
    "try:\n",
    "    # When imported as a module\n",
    "    from ..model_components.IsaerftConfig import IsaerftConfig\n",
    "    from ..model_components.IsaerftPeft import IsaerftPeft\n",
    "except ImportError:\n",
    "    # When run directly as a script\n",
    "    import sys\n",
    "    import os\n",
    "    # Add the parent directory to the path\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    from ..model_components.IsaerftConfig import IsaerftConfig\n",
    "    from ..model_components.IsaerftPeft import IsaerftPeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b69098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "# YOU HAVE TO SET CUDA_VISIBLE_DEVICES BEFORE DOING ANY IMPORTS OF cuda-related packages! https://discuss.pytorch.org/t/setting-visible-devices-with-distributed-data-parallel/93230\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true' # if the process hangs, turn this to false.\n",
    "import torch\n",
    "print(torch.cuda.device_count())  # Should print 1, but doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0fb046c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import libraries\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from sae_lens import HookedSAETransformer, SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc3115a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import our custom ISAERFT components\n",
    "\n",
    "# When run directly as a script\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to the path\n",
    "sys.path.append(os.path.dirnam(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n",
    "from model_components.IsaerftConfig import IsaerftConfig\n",
    "from model_components.IsaerftPeft import IsaerftPeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7f0179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import os\n",
    "# YOU HAVE TO SET CUDA_VISIBLE_DEVICES BEFORE DOING ANY IMPORTS OF cuda-related packages! https://discuss.pytorch.org/t/setting-visible-devices-with-distributed-data-parallel/93230\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true' # if the process hangs, turn this to false.\n",
    "import torch\n",
    "print(torch.cuda.device_count())  # Should print 1, but doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f71145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import libraries\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from sae_lens import HookedSAETransformer, SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f1d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import our custom ISAERFT components\n",
    "\n",
    "# When run directly as a script\n",
    "import sys\n",
    "import os\n",
    "# Add the parent directory to the path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n",
    "from model_components.IsaerftConfig import IsaerftConfig\n",
    "from model_components.IsaerftPeft import IsaerftPeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68d82533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HUGGINGFACE_WRITE_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0432753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Load dataset\n",
    "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a60d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Define the model\n",
    "model_name = \"google/gemma-2-2b\" \n",
    "simpler_model_name = model_name.split('/')[1]\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the actual device that CUDA is using\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device that's actually being used\n",
    "    device = f\"cuda:{torch.cuda.current_device()}\"\n",
    "else:\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "assert 'cuda' in device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84f57db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Model to fine-tune\n",
    "hooked_sae_transformer = HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float32,\n",
    "    # device_map=device\n",
    ").to(device)\n",
    "assert isinstance(hooked_sae_transformer, HookedSAETransformer)\n",
    "# model.config.use_cache = False\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b-it')#(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b85b55b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# chat_template = \"\"\"{% for message in messages %}\n",
    "# {% if message['role'] == 'user' %}\n",
    "# ### Instruction:\n",
    "# {{ message['content'] }}\n",
    "# {% elif message['role'] == 'assistant' %}\n",
    "# ### Response:\n",
    "# {{ message['content'] }}\n",
    "# {% endif %}\n",
    "# {% endfor %}\n",
    "# {% if add_generation_prompt %}\n",
    "# ### Response:\n",
    "# {% endif %}\"\"\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d855b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# non_hooked_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9355d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# del non_hooked_model\n",
    "# chat = [\n",
    "#     { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)\n",
    "# #%%\n",
    "# _, tokenizer = setup_chat_format(non_hooked_model, tokenizer) # TODO: Figure out if this is a problem that I'm not applying the chat format to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8cdc43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply ISAERFT to the model\n",
    "from sae_lens import SAE\n",
    "\n",
    "example_releases_ids = {\n",
    "    \"EleutherAI/pythia-70m-deduped\":(\"pythia-70m-deduped-res-sm\", \"blocks.4.hook_resid_post\"),\n",
    "    \"google/gemma-2-2b\": (\"gemma-scope-2b-pt-res-canonical\",\"layer_20/width_16k/canonical\")} \n",
    "isaerft_config = IsaerftConfig(\n",
    "    target_hooks=[\n",
    "        example_releases_ids[model_name],\n",
    "    ],\n",
    "    depth=-1  # Bias-only for simplicity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4dea9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9382152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "run_name=f\"run-{simpler_model_name}-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "finetune_name = f\"{simpler_model_name.upper()}-FT-ORPO-ISAERFT_\"+run_name\n",
    "finetune_tags = [\"smol-course\", \"module_1\", \"isaerft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc53a549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cs29824/matthew/interpretable-fine-tuning/src/training/checking_properties/wandb/run-20250303_205303-dhwogp48</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep/runs/dhwogp48' target=\"_blank\">revived-sweep-1</a></strong> to <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep/sweeps/icit3yqj' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep/sweeps/icit3yqj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep/sweeps/icit3yqj' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep/sweeps/icit3yqj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep/runs/dhwogp48' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/orpo-isaerft-sweep/runs/dhwogp48</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/62 : < :, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='62' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3/62 00:20 < 20:09, 0.05 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%\n",
    "# Define sweep configuration\n",
    "import wandb\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',  # Random search over the parameter space\n",
    "    'metric': {\n",
    "        'name': 'eval/rewards/margins',  # Metric to optimize\n",
    "        'goal': 'maximize'    # We want to maximize the reward margin\n",
    "    },\n",
    "    'parameters': {\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-6,\n",
    "            'max': 5e-5\n",
    "        },\n",
    "        'beta': {\n",
    "            'values': [0.05, 0.1, 0.15, 0.2]  # Different beta values to try\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"orpo-isaerft-sweep\")\n",
    "\n",
    "# Define the training function\n",
    "def train_model(config=None):\n",
    "    # The config parameter should be properly passed by wandb.agent\n",
    "    # We shouldn't need to create a default here if the sweep is configured correctly\n",
    "    with wandb.init(project=\"orpo-isaerft-sweep\", tags=finetune_tags, config=config) as run:\n",
    "        # Get the config from the wandb run\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Create a descriptive name based on the actual config values\n",
    "        run_name = f\"{simpler_model_name}-lr{config.learning_rate:.1e}-beta{config.beta}\"\n",
    "        # Update the run name\n",
    "        wandb.run.name = run_name\n",
    "        wandb.run.save()\n",
    "        \n",
    "        assert isinstance(hooked_sae_transformer, HookedSAETransformer)\n",
    "        hooked_sae_transformer.reset_saes()\n",
    "        # Apply the ISAERFT adapter\n",
    "        model = IsaerftPeft(hooked_sae_transformer, isaerft_config)\n",
    "        \n",
    "        # Use the same naming convention for saved files\n",
    "        current_run_name = f\"{run_name}-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "        current_finetune_name = f\"{simpler_model_name.upper()}-FT-ORPO-ISAERFT_{current_run_name}\"\n",
    "        \n",
    "        # Train model with ORPO\n",
    "        orpo_args = ORPOConfig(\n",
    "            # Use the learning rate from sweep config\n",
    "            learning_rate=config.learning_rate,\n",
    "            # Linear learning rate decay over training\n",
    "            lr_scheduler_type=\"linear\",\n",
    "            # Maximum combined length of prompt + completion\n",
    "            max_length=1024,\n",
    "            # Maximum length for input prompts\n",
    "            max_prompt_length=512,\n",
    "            # Controls weight of the odds ratio loss (Œª in paper) - from sweep config\n",
    "            beta=config.beta,\n",
    "            # Batch size for training\n",
    "            per_device_train_batch_size=2,\n",
    "            per_device_eval_batch_size=2,\n",
    "            # Helps with training stability by accumulating gradients before updating\n",
    "            gradient_accumulation_steps=8,\n",
    "            # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
    "            optim=\"paged_adamw_8bit\" if (\"cuda\" in device) else \"adamw_torch\",\n",
    "            # When to run evaluation\n",
    "            eval_strategy=\"steps\",\n",
    "            # Evaluate every 20% of training\n",
    "            eval_steps=0.2,\n",
    "            # Log metrics every step\n",
    "            logging_steps=1,\n",
    "            # Gradual learning rate warmup\n",
    "            warmup_steps=10,\n",
    "            # Use wandb for logging\n",
    "            report_to=\"wandb\",\n",
    "            # Where to save model/checkpoints\n",
    "            output_dir=f\"./results/orpo_isaerft/{current_run_name}\",\n",
    "            # Enable MPS (Metal Performance Shaders) if available\n",
    "            use_mps_device=device == \"mps\",\n",
    "            hub_model_id=current_finetune_name,\n",
    "            # Training for a shorter time for this example\n",
    "            num_train_epochs=1,\n",
    "            # Ensure device placement is correct\n",
    "            no_cuda=False,\n",
    "            dataloader_pin_memory=True,\n",
    "            dataloader_drop_last=True,\n",
    "            dataloader_num_workers=4,\n",
    "        )\n",
    "        \n",
    "        # Create the trainer\n",
    "        trainer = ORPOTrainer(\n",
    "            model=model,\n",
    "            args=orpo_args,\n",
    "            train_dataset=dataset[\"train\"].select(range(1000)),\n",
    "            eval_dataset=dataset[\"test\"].select(range(100)),\n",
    "            processing_class=tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        trainer.train()\n",
    "        \n",
    "        # Save the model\n",
    "        trainer.save_model(f\"./results/{current_finetune_name}\")\n",
    "        \n",
    "        # Only push the best model to hub (optional)\n",
    "        # You could add logic here to only push if this is the best run so far\n",
    "        try:\n",
    "            print(\"Pushing model to hub...\")\n",
    "            trainer.push_to_hub(tags=finetune_tags + [f\"lr_{config.learning_rate}\", f\"beta_{config.beta}\"])\n",
    "            print(\"Successfully pushed to hub!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error pushing to hub: {str(e)}\")\n",
    "            # Alternative manual push\n",
    "            print(\"Attempting manual push...\")\n",
    "            model.push_to_hub(current_finetune_name, tags=finetune_tags + [f\"lr_{config.learning_rate}\", f\"beta_{config.beta}\"])\n",
    "            tokenizer.push_to_hub(current_finetune_name, tags=finetune_tags + [f\"lr_{config.learning_rate}\", f\"beta_{config.beta}\"])\n",
    "            print(\"Manual push completed!\")\n",
    "\n",
    "# Run the sweep\n",
    "wandb.agent(sweep_id, train_model, count=10)  # Run 10 experiments\n",
    "\n",
    "print(\"## üíê Sweep completed!\")\n",
    "print(\"You've successfully run a hyperparameter sweep for fine-tuning a HookedSAETransformer with ISAERFT!\")\n",
    "print(\"Check your wandb dashboard to see the results and find the best hyperparameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44335b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sae_lens import HookedSAETransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aca6980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Import our custom ISAERFT components\n",
    "try:\n",
    "    # When imported as a module\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft\n",
    "except ImportError:\n",
    "    # When run directly as a script\n",
    "    import sys\n",
    "    import os\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a69508d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "login(token=os.environ['HUGGINGFACE_WRITE_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7759aa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Define the model\n",
    "model_name = \"google/gemma-2-2b\"\n",
    "\n",
    "# Get the actual device that CUDA is using\n",
    "if torch.cuda.is_available():\n",
    "    device = f\"cuda:{torch.cuda.current_device()}\"\n",
    "else:\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "assert 'cuda' in device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bc01e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f001eab2834c3b876cfab32496e15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "# Load model\n",
    "hooked_sae_transformer = HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    ").to(device)\n",
    "assert isinstance(hooked_sae_transformer, HookedSAETransformer)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('google/gemma-2-2b-it')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
