{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafc45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5e1d452",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import libraries\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from sae_lens import HookedSAETransformer, SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc56089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import our custom ISAERFT components\n",
    "try:\n",
    "    # When imported as a module\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft\n",
    "except ImportError:\n",
    "    # When run directly as a script\n",
    "    import sys\n",
    "    import os\n",
    "    # Add the parent directory to the path\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94a7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only GPU 0\n",
    "import torch\n",
    "print(torch.cuda.device_count())  # Should print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f3d7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HUGGINGFACE_WRITE_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000761ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Load dataset\n",
    "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c273ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Define the model\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\" \n",
    "\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "assert 'cuda' in device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50bc8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Model to fine-tune\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float32,\n",
    "    # device_map=device\n",
    ").to(device)\n",
    "# model.config.use_cache = False\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d6786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# non_hooked_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7bf3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# del non_hooked_model\n",
    "# chat = [\n",
    "#     { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)\n",
    "# #%%\n",
    "# _, tokenizer = setup_chat_format(non_hooked_model, tokenizer) # TODO: Figure out if this is a problem that I'm not applying the chat format to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f09f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply ISAERFT to the model\n",
    "from sae_lens import SAE\n",
    "\n",
    "release = \"pythia-70m-deduped-res-sm\"\n",
    "sae_id = \"blocks.4.hook_resid_post\"\n",
    "isaerft_config = IsaerftConfig(\n",
    "    target_hooks=[\n",
    "        (release, sae_id),\n",
    "    ],\n",
    "    depth=-1  # Bias-only for simplicity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14446746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply the ISAERFT adapter\n",
    "model = IsaerftPeft(model, isaerft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9949ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe5ec79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply the ISAERFT adapter\n",
    "model = IsaerftPeft(model, isaerft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea8cb1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply the ISAERFT adapter\n",
    "model = IsaerftPeft(model, isaerft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fde5be4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_components.IsaerftPeft.IsaerftPeft"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c4ca1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sae_lens.analysis.hooked_sae_transformer.HookedSAETransformer"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "type(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06300278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method HookedSAETransformer.saes of HookedSAETransformer(\n",
      "  (embed): Embed()\n",
      "  (hook_embed): HookPoint()\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_post): SAE(\n",
      "        (activation_fn): ReLU()\n",
      "        (hook_sae_input): HookPoint()\n",
      "        (hook_sae_acts_pre): HookPoint()\n",
      "        (hook_sae_acts_post): HookPoint()\n",
      "        (hook_sae_output): HookPoint()\n",
      "        (hook_sae_recons): HookPoint()\n",
      "        (hook_sae_error): HookPoint()\n",
      "      )\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (ln1): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (ln2): LayerNormPre(\n",
      "        (hook_scale): HookPoint()\n",
      "        (hook_normalized): HookPoint()\n",
      "      )\n",
      "      (attn): Attention(\n",
      "        (hook_k): HookPoint()\n",
      "        (hook_q): HookPoint()\n",
      "        (hook_v): HookPoint()\n",
      "        (hook_z): HookPoint()\n",
      "        (hook_attn_scores): HookPoint()\n",
      "        (hook_pattern): HookPoint()\n",
      "        (hook_result): HookPoint()\n",
      "        (hook_rot_k): HookPoint()\n",
      "        (hook_rot_q): HookPoint()\n",
      "      )\n",
      "      (mlp): MLP(\n",
      "        (hook_pre): HookPoint()\n",
      "        (hook_post): HookPoint()\n",
      "      )\n",
      "      (hook_attn_in): HookPoint()\n",
      "      (hook_q_input): HookPoint()\n",
      "      (hook_k_input): HookPoint()\n",
      "      (hook_v_input): HookPoint()\n",
      "      (hook_mlp_in): HookPoint()\n",
      "      (hook_attn_out): HookPoint()\n",
      "      (hook_mlp_out): HookPoint()\n",
      "      (hook_resid_pre): HookPoint()\n",
      "      (hook_resid_post): HookPoint()\n",
      "    )\n",
      "  )\n",
      "  (ln_final): LayerNormPre(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (unembed): Unembed()\n",
      ")>"
     ]
    }
   ],
   "source": [
    "model.model.saes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cea784d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Check device placement of model components\n",
    "def check_device():\n",
    "    print(\"Checking device placement of model components...\")\n",
    "    model_device = next(model.base_model.parameters()).device\n",
    "    import torch\n",
    "    print(f\"{torch.device(device)=}\")\n",
    "    print(f\"{model_device=}\")\n",
    "    assert model_device == torch.device(device)\n",
    "    # Check base model\n",
    "    print(f\"Base model device: {model_device}\")\n",
    "\n",
    "    assert(all(p[1].device == model_device for p in model.named_parameters()))\n",
    "    text = \"Hello, world!\"\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_ids)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(tokenizer.apply_chat_template(text, tokenize=False, add_generation_prompt=True))\n",
    "    # Run an example through the model to verify forward pass\n",
    "    print(\"Testing model forward pass...\")\n",
    "\n",
    "    # Create a simple test input\n",
    "    test_chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Write a hello world program\"}\n",
    "    ]\n",
    "    test_prompt = tokenizer.apply_chat_template(test_chat, tokenize=False, add_generation_prompt=True)\n",
    "    print(test_prompt)\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "    print(inputs)\n",
    "    # Run forward pass\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "    print(\"Forward pass successful!\")\n",
    "    print(f\"Output shape: {outputs.logits.shape}\")\n",
    "    print(f\"Output device: {outputs.logits.device}\")\n",
    "    assert outputs.logits.device == model_device, \"Output device doesn't match model device\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aab993ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88cc347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"PYTHIA-FT-ORPO-ISAERFT\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\", \"isaerft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68efe546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Train model with ORPO\n",
    "orpo_args = ORPOConfig(\n",
    "    # Small learning rate to prevent catastrophic forgetting\n",
    "    learning_rate=8e-6,\n",
    "    # Linear learning rate decay over training\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    # Maximum combined length of prompt + completion\n",
    "    max_length=1024,\n",
    "    # Maximum length for input prompts\n",
    "    max_prompt_length=512,\n",
    "    # Controls weight of the odds ratio loss (λ in paper)\n",
    "    beta=0.1,\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    # Helps with training stability by accumulating gradients before updating\n",
    "    gradient_accumulation_steps=8,\n",
    "    # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
    "    optim=\"paged_adamw_8bit\" if (\"cuda\" in device)else \"adamw_torch\",\n",
    "    # When to run evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    # Evaluate every 20% of training\n",
    "    eval_steps=0.2,\n",
    "    # Log metrics every step\n",
    "    logging_steps=1,\n",
    "    # Gradual learning rate warmup\n",
    "    warmup_steps=10,\n",
    "    # Disable external logging\n",
    "    report_to=\"wandb\",\n",
    "    # Where to save model/checkpoints\n",
    "    output_dir=\"./results/\",\n",
    "    # Enable MPS (Metal Performance Shaders) if available\n",
    "    use_mps_device=device == \"mps\",\n",
    "    hub_model_id=finetune_name,\n",
    "    # Training for a shorter time for this example\n",
    "    num_train_epochs=(1/4*.25),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8165cf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run at 0x7fec5f981a50>"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cs29824/matthew/interpretable-fine-tuning/src/training/wandb/run-20250227_060056-o9c70arr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft/runs/o9c70arr' target=\"_blank\">run-20250227-0600-0cc45d</a></strong> to <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft/runs/o9c70arr' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft/runs/o9c70arr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%\n",
    "# Initialize wandb\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "wandb.finish()\n",
    "wandb.login(key=os.environ['WANDB_KEY'])\n",
    "\n",
    "wandb.init(\n",
    "    project=\"gemma-2-2b-orpo-isaerft\",\n",
    "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M')}-{uuid.uuid4().hex[:6]}\",\n",
    "    tags=finetune_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b355b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Initialize wandb\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "wandb.finish()\n",
    "wandb.login(key=os.environ['WANDB_KEY'])\n",
    "\n",
    "wandb.init(\n",
    "    project=\"pythia70m-orpo-isaerft\",\n",
    "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M')}-{uuid.uuid4().hex[:6]}\",\n",
    "    tags=finetune_tags\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
