{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c74ea9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b5b5935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import libraries\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from sae_lens import HookedSAETransformer, SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368b3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import our custom ISAERFT components\n",
    "try:\n",
    "    # When imported as a module\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft\n",
    "except ImportError:\n",
    "    # When run directly as a script\n",
    "    import sys\n",
    "    import os\n",
    "    # Add the parent directory to the path\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70c15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HUGGINGFACE_WRITE_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54261621",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Load dataset\n",
    "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf71d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Define the model\n",
    "model_name = \"google/gemma-2-2b\" # don't change this, I needed to do jank things to the tokenizer\n",
    "\n",
    "device = (\n",
    "    \"cuda:0\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "assert 'cuda' in device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51ddde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Model to fine-tune\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    ").to(device)\n",
    "# model.config.use_cache = False\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ba1612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "non_hooked_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95446c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# del non_hooked_model\n",
    "# chat = [\n",
    "#     { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)\n",
    "# #%%\n",
    "# _, tokenizer = setup_chat_format(non_hooked_model, tokenizer) # TODO: Figure out if this is a problem that I'm not applying the chat format to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e808378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply ISAERFT to the model\n",
    "isaerft_config = IsaerftConfig(\n",
    "    target_hooks=[\n",
    "        (\"gemma-scope-2b-pt-res-canonical\", \"layer_20/width_16k/canonical\"),\n",
    "    ],\n",
    "    depth=-1  # Bias-only for simplicity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c828752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply the ISAERFT adapter\n",
    "model = IsaerftPeft(model, isaerft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7520b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53cb985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"GEMMA-2-2B-FT-ORPO-ISAERFT\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\", \"isaerft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36b745b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Train model with ORPO\n",
    "orpo_args = ORPOConfig(\n",
    "    # Small learning rate to prevent catastrophic forgetting\n",
    "    learning_rate=8e-6,\n",
    "    # Linear learning rate decay over training\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    # Maximum combined length of prompt + completion\n",
    "    max_length=1024,\n",
    "    # Maximum length for input prompts\n",
    "    max_prompt_length=512,\n",
    "    # Controls weight of the odds ratio loss (Î» in paper)\n",
    "    beta=0.1,\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    # Helps with training stability by accumulating gradients before updating\n",
    "    gradient_accumulation_steps=4,\n",
    "    # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
    "    optim=\"paged_adamw_8bit\" if (\"cuda\" in device)else \"adamw_torch\",\n",
    "    # When to run evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    # Evaluate every 20% of training\n",
    "    eval_steps=0.2,\n",
    "    # Log metrics every step\n",
    "    logging_steps=1,\n",
    "    # Gradual learning rate warmup\n",
    "    warmup_steps=10,\n",
    "    # Disable external logging\n",
    "    report_to=\"wandb\",\n",
    "    # Where to save model/checkpoints\n",
    "    output_dir=\"./results/\",\n",
    "    # Enable MPS (Metal Performance Shaders) if available\n",
    "    use_mps_device=device == \"mps\",\n",
    "    hub_model_id=finetune_name,\n",
    "    # Training for a shorter time for this example\n",
    "    num_train_epochs=(1/4*.25),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "727dc175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run at 0x7f9741e7e6d0>"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cs29824/matthew/interpretable-fine-tuning/src/training/wandb/run-20250227_045752-j6u9hbmw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft/runs/j6u9hbmw' target=\"_blank\">run-20250227-0457-8e3218</a></strong> to <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft/runs/j6u9hbmw' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/gemma-2-2b-orpo-isaerft/runs/j6u9hbmw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%\n",
    "# Initialize wandb\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "wandb.finish()\n",
    "wandb.login(key=os.environ['WANDB_KEY'])\n",
    "\n",
    "wandb.init(\n",
    "    project=\"gemma-2-2b-orpo-isaerft\",\n",
    "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M')}-{uuid.uuid4().hex[:6]}\",\n",
    "    tags=finetune_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94b52fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "type(dataset['train'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b5b4d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Create the trainer\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"].select(range(10)),\n",
    "    eval_dataset=dataset[\"test\"].select(range(10)),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8de89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb107e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Train model with ORPO\n",
    "orpo_args = ORPOConfig(\n",
    "    # Small learning rate to prevent catastrophic forgetting\n",
    "    learning_rate=8e-6,\n",
    "    # Linear learning rate decay over training\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    # Maximum combined length of prompt + completion\n",
    "    max_length=1024,\n",
    "    # Maximum length for input prompts\n",
    "    max_prompt_length=512,\n",
    "    # Controls weight of the odds ratio loss (Î» in paper)\n",
    "    beta=0.1,\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    # Helps with training stability by accumulating gradients before updating\n",
    "    gradient_accumulation_steps=8,\n",
    "    # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
    "    optim=\"paged_adamw_8bit\" if (\"cuda\" in device)else \"adamw_torch\",\n",
    "    # When to run evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    # Evaluate every 20% of training\n",
    "    eval_steps=0.2,\n",
    "    # Log metrics every step\n",
    "    logging_steps=1,\n",
    "    # Gradual learning rate warmup\n",
    "    warmup_steps=10,\n",
    "    # Disable external logging\n",
    "    report_to=\"wandb\",\n",
    "    # Where to save model/checkpoints\n",
    "    output_dir=\"./results/\",\n",
    "    # Enable MPS (Metal Performance Shaders) if available\n",
    "    use_mps_device=device == \"mps\",\n",
    "    hub_model_id=finetune_name,\n",
    "    # Training for a shorter time for this example\n",
    "    num_train_epochs=(1/4*.25),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58445fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Initialize wandb\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "wandb.finish()\n",
    "wandb.login(key=os.environ['WANDB_KEY'])\n",
    "\n",
    "wandb.init(\n",
    "    project=\"gemma-2-2b-orpo-isaerft\",\n",
    "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M')}-{uuid.uuid4().hex[:6]}\",\n",
    "    tags=finetune_tags\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
