{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9a4714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "# YOU HAVE TO SET CUDA_VISIBLE_DEVICES BEFORE DOING ANY IMPORTS OF cuda-related packages! https://discuss.pytorch.org/t/setting-visible-devices-with-distributed-data-parallel/93230\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] ='0'\n",
    "import torch\n",
    "print(torch.cuda.device_count())  # Should print 1, but doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd26a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import libraries\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format\n",
    "from sae_lens import HookedSAETransformer, SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "376ebdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Import our custom ISAERFT components\n",
    "try:\n",
    "    # When imported as a module\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft\n",
    "except ImportError:\n",
    "    # When run directly as a script\n",
    "    import sys\n",
    "    import os\n",
    "    # Add the parent directory to the path\n",
    "    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "    from model_components.IsaerftConfig import IsaerftConfig\n",
    "    from model_components.IsaerftPeft import IsaerftPeft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d07f038e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Authenticate to Hugging Face\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HUGGINGFACE_WRITE_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17455c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Load dataset\n",
    "dataset = load_dataset(path=\"trl-lib/ultrafeedback_binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be32249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Define the model\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\" \n",
    "\n",
    "# Get the actual device that CUDA is using\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device that's actually being used\n",
    "    device = f\"cuda:{torch.cuda.current_device()}\"\n",
    "else:\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "assert 'cuda' in device\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c720a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Model to fine-tune\n",
    "model = HookedSAETransformer.from_pretrained(\n",
    "    model_name,\n",
    "    # torch_dtype=torch.float32,\n",
    "    # device_map=device\n",
    ").to(device)\n",
    "# model.config.use_cache = False\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9a29177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "chat_template = \"\"\"{% for message in messages %}\n",
    "{% if message['role'] == 'user' %}\n",
    "### Instruction:\n",
    "{{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}\n",
    "### Response:\n",
    "{{ message['content'] }}\n",
    "{% endif %}\n",
    "{% endfor %}\n",
    "{% if add_generation_prompt %}\n",
    "### Response:\n",
    "{% endif %}\"\"\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer.chat_template = chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9d5176",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# non_hooked_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b\").to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1904390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# del non_hooked_model\n",
    "# chat = [\n",
    "#     { \"role\": \"user\", \"content\": \"Write a hello world program\" },\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "# print(prompt)\n",
    "# #%%\n",
    "# _, tokenizer = setup_chat_format(non_hooked_model, tokenizer) # TODO: Figure out if this is a problem that I'm not applying the chat format to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e15d4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply ISAERFT to the model\n",
    "from sae_lens import SAE\n",
    "\n",
    "release = \"pythia-70m-deduped-res-sm\"\n",
    "sae_id = \"blocks.4.hook_resid_post\"\n",
    "isaerft_config = IsaerftConfig(\n",
    "    target_hooks=[\n",
    "        (release, sae_id),\n",
    "    ],\n",
    "    depth=-1  # Bias-only for simplicity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64c2d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Apply the ISAERFT adapter\n",
    "model = IsaerftPeft(model, isaerft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d3c352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Check device placement of model components\n",
    "def check_device():\n",
    "    print(\"Checking device placement of model components...\")\n",
    "    model_device = next(model.base_model.parameters()).device\n",
    "    import torch\n",
    "    print(f\"{torch.device(device)=}\")\n",
    "    print(f\"{model_device=}\")\n",
    "    assert model_device == torch.device(device)\n",
    "    # Check base model\n",
    "    print(f\"Base model device: {model_device}\")\n",
    "\n",
    "    assert(all(p[1].device == model_device for p in model.named_parameters()))\n",
    "    text = \"Hello, world!\"\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "    print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(input_ids)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(tokenizer.apply_chat_template(text, tokenize=False, add_generation_prompt=True))\n",
    "    # Run an example through the model to verify forward pass\n",
    "    print(\"Testing model forward pass...\")\n",
    "\n",
    "    # Create a simple test input\n",
    "    test_chat = [\n",
    "        {\"role\": \"user\", \"content\": \"Write a hello world program\"}\n",
    "    ]\n",
    "    test_prompt = tokenizer.apply_chat_template(test_chat, tokenize=False, add_generation_prompt=True)\n",
    "    print(test_prompt)\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(device)\n",
    "    print(inputs)\n",
    "    # Run forward pass\n",
    "    outputs = model(**inputs)\n",
    "    print(outputs)\n",
    "    print(\"Forward pass successful!\")\n",
    "    print(f\"Output shape: {outputs.logits.shape}\")\n",
    "    print(f\"Output device: {outputs.logits.device}\")\n",
    "    assert outputs.logits.device == model_device, \"Output device doesn't match model device\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cd7e4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "# model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99beb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"PYTHIA-FT-ORPO-ISAERFT\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\", \"isaerft\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53e7b50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Train model with ORPO\n",
    "orpo_args = ORPOConfig(\n",
    "    # Small learning rate to prevent catastrophic forgetting\n",
    "    learning_rate=8e-6,\n",
    "    # Linear learning rate decay over training\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    # Maximum combined length of prompt + completion\n",
    "    max_length=1024,\n",
    "    # Maximum length for input prompts\n",
    "    max_prompt_length=512,\n",
    "    # Controls weight of the odds ratio loss (Î» in paper)\n",
    "    beta=0.1,\n",
    "    # Batch size for training\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    # Helps with training stability by accumulating gradients before updating\n",
    "    gradient_accumulation_steps=8,\n",
    "    # Memory-efficient optimizer for CUDA, falls back to adamw_torch for CPU/MPS\n",
    "    optim=\"paged_adamw_8bit\" if (\"cuda\" in device)else \"adamw_torch\",\n",
    "    # When to run evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    # Evaluate every 20% of training\n",
    "    eval_steps=0.2,\n",
    "    # Log metrics every step\n",
    "    logging_steps=1,\n",
    "    # Gradual learning rate warmup\n",
    "    warmup_steps=10,\n",
    "    # Disable external logging\n",
    "    report_to=\"wandb\",\n",
    "    # Where to save model/checkpoints\n",
    "    output_dir=\"./results/orpo_isaerft\",\n",
    "    # Enable MPS (Metal Performance Shaders) if available\n",
    "    use_mps_device=device == \"mps\",\n",
    "    hub_model_id=finetune_name,\n",
    "    # Training for a shorter time for this example\n",
    "    num_train_epochs=(1/4*.25),\n",
    "    # Ensure device placement is correct\n",
    "    no_cuda=False,\n",
    "    dataloader_pin_memory=False,\n",
    "    dataloader_drop_last=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0374c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cfg',\n",
      "  SAEConfig(architecture='standard', d_in=512, d_sae=32768, activation_fn_str='relu', apply_b_dec_to_input=True, finetuning_scaling_factor=False, context_size=128, model_name='pythia-70m-deduped', hook_name='blocks.4.hook_resid_post', hook_layer=4, hook_head_index=None, prepend_bos=False, dataset_path='EleutherAI/the_pile_deduplicated', dataset_trust_remote_code=True, normalize_activations='none', dtype='torch.float32', device='cuda:0', sae_lens_training_version=None, activation_fn_kwargs={}, neuronpedia_id='pythia-70m-deduped/4-res-sm', model_from_pretrained_kwargs={}, seqpos_slice=(None,)))]"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "thesae = next(iter(model.saes.items()))[1]\n",
    "[(k,v) for k,v in thesae.__dict__.items() if hasattr(v, 'device')]\n",
    "# thesae.W_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "323b4d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run at 0x7f003d63e790>"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cs29824/matthew/interpretable-fine-tuning/src/training/wandb/run-20250228_030641-tiopeeyx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/matthewkhoriaty-northwestern-university/pythia70m-orpo-isaerft/runs/tiopeeyx' target=\"_blank\">run-20250228-0306-3ae54e</a></strong> to <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/pythia70m-orpo-isaerft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/pythia70m-orpo-isaerft' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/pythia70m-orpo-isaerft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/matthewkhoriaty-northwestern-university/pythia70m-orpo-isaerft/runs/tiopeeyx' target=\"_blank\">https://wandb.ai/matthewkhoriaty-northwestern-university/pythia70m-orpo-isaerft/runs/tiopeeyx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%\n",
    "# Initialize wandb\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "wandb.finish()\n",
    "wandb.login(key=os.environ['WANDB_KEY'])\n",
    "\n",
    "wandb.init(\n",
    "    project=\"pythia70m-orpo-isaerft\",\n",
    "    name=f\"run-{datetime.now().strftime('%Y%m%d-%H%M')}-{uuid.uuid4().hex[:6]}\",\n",
    "    tags=finetune_tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2f3f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Create the trainer\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"].select(range(100)),\n",
    "    eval_dataset=dataset[\"test\"].select(range(10)),\n",
    "    processing_class=tokenizer,\n",
    "    # peft_config=isaerft_config, # don't include this; it is one or the other: model is a HookedSAETransformer and peft_config is used to transform it, or model is an IsaerftPeft and no peft_config needed\n",
    "    # label_names=[\"labels\"],  # This is the standard label name for causal language models\n",
    "    # dataset_num_proc=1,\n",
    ")\n",
    "\n",
    "# Get the actual device the model is on\n",
    "model_device = next(model.parameters()).device\n",
    "print(f\"Model is on device: {model_device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a88d00bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=1, training_loss=63.85625076293945, metrics={'train_runtime': 2.3306, 'train_samples_per_second': 2.682, 'train_steps_per_second': 0.429, 'total_flos': 0.0, 'train_loss': 63.85625076293945, 'epoch': 0.16})"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/5 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/5 00:00 < 00:00, 15.39 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/5 00:00 < 00:00, 10.89 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/5 00:00 < 00:00, 12.23 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/1 : < :, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>63.856300</td>\n",
       "      <td>6.971425</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>21.144000</td>\n",
       "      <td>10.572000</td>\n",
       "      <td>-2.733371</td>\n",
       "      <td>-6.284737</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.551366</td>\n",
       "      <td>-62.847370</td>\n",
       "      <td>-27.333710</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5.843417</td>\n",
       "      <td>-11.280080</td>\n",
       "      <td>35.490234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>63.856300</td>\n",
       "      <td>6.971425</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>21.144000</td>\n",
       "      <td>10.572000</td>\n",
       "      <td>-2.733371</td>\n",
       "      <td>-6.284737</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>3.551366</td>\n",
       "      <td>-62.847370</td>\n",
       "      <td>-27.333710</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>5.843417</td>\n",
       "      <td>-11.280080</td>\n",
       "      <td>35.490234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%%\n",
    "# Train the model\n",
    "# b /home/cs29824/matthew/interpretable-fine-tuning/.venv/lib/python3.11/site-packages/transformer_lens/components/embed.py:34\n",
    "# import pdb;pdb.set_trace()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b77482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")\n",
    "\n",
    "# Finish wandb logging\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
