{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from jaxtyping import Float\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from torch import Tensor\n",
    "from transformer_lens import loading_from_pretrained\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "import torch.utils.checkpoint as checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4511,  0.0766,  0.2233,  1.1803, -0.8610, -0.1251,  0.9687,  0.0301,\n",
      "         0.7975,  0.1416])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "example_input = torch.randn(10)\n",
    "print(example_input)\n",
    "print(example_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4511,  0.0766,  0.2233,  1.1803, -0.8610, -0.1251,  0.9687,  0.0301,\n",
      "         0.7975,  0.1416], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class BiasOnly(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "\n",
    "# Example usage:\n",
    "bias_layer = BiasOnly(features=example_input.shape[0])\n",
    "output = bias_layer(example_input)  # Will add a learnable bias to each feature\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(torch.nn.Module):\n",
    "    def __init__(self, input_dim:int, hidden_layers:int, hidden_dim:int|None=None, activation=torch.nn.ReLU()):\n",
    "        \"\"\"A flexible residual neural network block that maintains input/output dimension compatibility.\n",
    "    \n",
    "        This block implements a residual connection of the form output = F(x) + x, where F is a configurable\n",
    "        neural network. The architecture supports various depths and can degenerate to a simple bias-only layer.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features. Must be positive. Output will have same dimension.\n",
    "            hidden_layers (int): Number of hidden layers in the network.\n",
    "                * -1: Creates a bias-only layer\n",
    "                * 0: Single linear transformation\n",
    "                * >0: Creates that many hidden layers with activation functions between them\n",
    "            hidden_dim (int, optional): Dimension of hidden layers. If None, uses input_dim.\n",
    "            activation (torch.nn.Module): Activation function to use between layers. Defaults to ReLU.\n",
    "        \n",
    "        Example:\n",
    "            >>> block = ResidualBlock(input_dim=512, hidden_layers=2, hidden_dim=1024)\n",
    "            >>> x = torch.randn(32, 512)  # batch_size=32, features=512\n",
    "            >>> output = block(x)  # Shape: (32, 512)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert input_dim > 0\n",
    "        assert hidden_layers >= -1\n",
    "        assert hidden_dim is None or hidden_dim > 0\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim if hidden_dim else input_dim\n",
    "        self.activation = activation\n",
    "        sequential = []\n",
    "        if hidden_layers == -1:\n",
    "            sequential.append(BiasOnly(input_dim))\n",
    "        else:\n",
    "            input_dims = [self.input_dim] + [self.hidden_dim] * hidden_layers\n",
    "            output_dims = [self.hidden_dim] * hidden_layers + [self.input_dim]\n",
    "            for i, (in_dim, out_dim) in enumerate(zip(input_dims, output_dims)): # plus one because zero hidden layers is just a forward from input to output\n",
    "                linear = torch.nn.Linear(in_dim, out_dim)\n",
    "                if i == len(input_dims) - 1:  # Final layer\n",
    "                    # Initialize final layer to zero for identity function behavior\n",
    "                    torch.nn.init.zeros_(linear.weight)\n",
    "                    torch.nn.init.zeros_(linear.bias)\n",
    "                else:\n",
    "                    # Xavier initialization for hidden layers\n",
    "                    torch.nn.init.xavier_uniform_(linear.weight)\n",
    "                    torch.nn.init.zeros_(linear.bias)\n",
    "                sequential.append(linear)\n",
    "                if i < hidden_layers - 1:\n",
    "                    sequential.append(activation)\n",
    "        self.sequential = torch.nn.Sequential(*sequential)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sequential(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_hook(sae_acts:Tensor, hook:HookPoint, residual_block:ResidualBlock) -> Tensor:\n",
    "    \"\"\"Runs the input through a trainable resnet (ResidualBlock).\n",
    "\n",
    "    Args:\n",
    "        sae_acts (Tensor): The SAE activations tensor, shape [batch, pos, features]\n",
    "        hook (HookPoint): The transformer-lens hook point\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The modified SAE activations modified by the trainable parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    return residual_block(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_steer(sae_acts: Tensor, hook:HookPoint) -> Tensor:\n",
    "    import pdb; pdb.set_trace()\n",
    "    pass\n",
    "    pass\n",
    "    return sae_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b28f93d3b5c46979c474b380cb3c926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained('google/gemma-2-2b', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feature we hope to edit (to be less) https://www.neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/8496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32296d8dbc464b969164d936740e5d88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae25, cfg_dict, sparsity = SAE.from_pretrained(release=\"gemma-scope-2b-pt-res-canonical\", sae_id=\"layer_25/width_16k/canonical\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world!\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8825530., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss = output.abs().sum()\n",
    "# print(loss)\n",
    "# loss.backward()\n",
    "# next(model.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model.forward(input_ids)\n",
    "# loss = output.abs().sum()\n",
    "# print(loss)\n",
    "# loss.backward()\n",
    "# next(model.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_sae(sae25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae25.add_hook(\"hook_sae_acts_post\", debug_steer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae25.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-20.8156, -13.4081, -16.8380,  ..., -18.4423, -16.2108, -20.8378],\n",
       "         [-19.9655,  -8.5803,   4.2300,  ...,  -9.0129,  -9.0134, -19.8179],\n",
       "         [-17.8220,   1.6014,   4.2865,  ...,  -8.9011,  -4.6994, -17.6331],\n",
       "         [-11.4105,   5.9845,  -0.3374,  ...,  -2.8015,   2.0687, -11.2620],\n",
       "         [-11.7846,  10.4476,   0.9414,  ...,  -5.8933,  -2.8285, -11.6628]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the case of this sae, the sae_acts shape is torch.Size([batch_size, seq_len, 16384])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_block = ResidualBlock(input_dim=16384, hidden_layers=-1).to(device)\n",
    "trainable_hook = partial(resid_hook, residual_block=residual_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(p.requires_grad for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae25.add_hook('hook_sae_acts_post', trainable_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(p.requires_grad for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(residual_block.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = output.abs().sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -16.7542,  469.0292, -253.8390,  ...,  251.0276,   72.6698,\n",
       "         240.4605], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(residual_block.parameters()).grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp_arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
