{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from jaxtyping import Float\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "from torch import Tensor\n",
    "from transformer_lens import loading_from_pretrained\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformers import AutoTokenizer\n",
    "import torch.utils.checkpoint as checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append('.')  # Add current directory to Python path\n",
    "from model_components import BiasOnly, ResidualBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4511,  0.0766,  0.2233,  1.1803, -0.8610, -0.1251,  0.9687,  0.0301,\n",
      "         0.7975,  0.1416])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "example_input = torch.randn(10)\n",
    "print(example_input)\n",
    "print(example_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4511,  0.0766,  0.2233,  1.1803, -0.8610, -0.1251,  0.9687,  0.0301,\n",
      "         0.7975,  0.1416], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "bias_layer = BiasOnly(features=example_input.shape[0])\n",
    "output = bias_layer(example_input)  # Will add a learnable bias to each feature\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resid_hook(sae_acts:Tensor, hook:HookPoint, residual_block:ResidualBlock) -> Tensor:\n",
    "    \"\"\"Runs the input through a trainable resnet (ResidualBlock).\n",
    "\n",
    "    Args:\n",
    "        sae_acts (Tensor): The SAE activations tensor, shape [batch, pos, features]\n",
    "        hook (HookPoint): The transformer-lens hook point\n",
    "\n",
    "    Returns:\n",
    "        Tensor: The modified SAE activations modified by the trainable parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    return residual_block(sae_acts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_steer(sae_acts: Tensor, hook:HookPoint) -> Tensor:\n",
    "    import pdb; pdb.set_trace()\n",
    "    pass\n",
    "    pass\n",
    "    return sae_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/gemma-2-2b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constant Setting center_unembed=False instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b28f93d3b5c46979c474b380cb3c926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained('google/gemma-2-2b', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example feature we hope to edit (to be less) https://www.neuronpedia.org/gemma-2-2b/25-gemmascope-res-16k/8496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_lens import SAE\n",
    "\n",
    "sae25, cfg_dict, sparsity = SAE.from_pretrained(release=\"gemma-scope-2b-pt-res-canonical\", sae_id=\"layer_25/width_16k/canonical\", device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'architecture': 'jumprelu', 'd_in': 2304, 'd_sae': 16384, 'dtype': 'float32', 'model_name': 'gemma-2-2b', 'hook_name': 'blocks.25.hook_resid_post', 'hook_layer': 25, 'hook_head_index': None, 'activation_fn_str': 'relu', 'finetuning_scaling_factor': False, 'sae_lens_training_version': None, 'prepend_bos': True, 'dataset_path': 'monology/pile-uncopyrighted', 'context_size': 1024, 'dataset_trust_remote_code': True, 'apply_b_dec_to_input': False, 'normalize_activations': None, 'device': 'cpu', 'neuronpedia_id': 'gemma-2-2b/25-gemmascope-res-16k'}\n"
     ]
    }
   ],
   "source": [
    "print(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "isinstance(sae25, torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE(\n",
      "  (activation_fn): ReLU()\n",
      "  (hook_sae_input): HookPoint()\n",
      "  (hook_sae_acts_pre): HookPoint()\n",
      "  (hook_sae_acts_post): HookPoint()\n",
      "  (hook_sae_output): HookPoint()\n",
      "  (hook_sae_recons): HookPoint()\n",
      "  (hook_sae_error): HookPoint()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(sae25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: torch.Size([16384])\n",
      "b_enc: torch.Size([16384])\n",
      "W_dec: torch.Size([16384, 2304])\n",
      "W_enc: torch.Size([2304, 16384])\n",
      "b_dec: torch.Size([2304])\n"
     ]
    }
   ],
   "source": [
    "for name, param in sae25.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, world!\"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8825530., device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# loss = output.abs().sum()\n",
    "# print(loss)\n",
    "# loss.backward()\n",
    "# next(model.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = model.forward(input_ids)\n",
    "# loss = output.abs().sum()\n",
    "# print(loss)\n",
    "# loss.backward()\n",
    "# next(model.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_sae(sae25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sae25.add_hook(\"hook_sae_acts_post\", debug_steer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae25.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-20.8156, -13.4081, -16.8380,  ..., -18.4423, -16.2108, -20.8378],\n",
       "         [-19.9655,  -8.5803,   4.2300,  ...,  -9.0129,  -9.0134, -19.8179],\n",
       "         [-17.8220,   1.6014,   4.2865,  ...,  -8.9011,  -4.6994, -17.6331],\n",
       "         [-11.4105,   5.9845,  -0.3374,  ...,  -2.8015,   2.0687, -11.2620],\n",
       "         [-11.7846,  10.4476,   0.9414,  ...,  -5.8933,  -2.8285, -11.6628]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the case of this sae, the sae_acts shape is torch.Size([batch_size, seq_len, 16384])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_block = ResidualBlock(input_dim=16384, hidden_layers=-1).to(device)\n",
    "trainable_hook = partial(resid_hook, residual_block=residual_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(p.requires_grad for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae25.add_hook('hook_sae_acts_post', trainable_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(p.requires_grad for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.forward(input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(residual_block.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = output.abs().sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -16.7542,  469.0292, -253.8390,  ...,  251.0276,   72.6698,\n",
       "         240.4605], device='cuda:0')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(residual_block.parameters()).grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp_arena",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
